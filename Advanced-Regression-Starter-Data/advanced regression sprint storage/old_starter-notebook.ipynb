{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**YOUR NAME, YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Spain Electricity Shortfall Challenge\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np #numerical computations. large, multi-dimensional arrays and matrices\n",
    "import pandas as pd #data manipulation and analysis\n",
    "import matplotlib.pyplot as plt #\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "import seaborn as sns #data visualization library provides a high-level interface for creating statistical graphics\n",
    "import random\n",
    "from IPython.display import display, Image\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression #work with a classification algorithm like Logistic Regression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "PARAMETER_CONSTANT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./df_train.csv') #Load the train CSV file into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f89244",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032b27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n",
    "# View the first 10 rows of the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b85e19",
   "metadata": {},
   "source": [
    "Observations: 8763 rows and 49 columns\n",
    "There is one unnamed column to be dropped in data engineering section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    }
   },
   "outputs": [],
   "source": [
    "# look at data statistics\n",
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #The info command confirms our categorical and numerical features. outputs the number of non-null entries in each column.\n",
    "#As such, we can be certain that our data has missing values if columns have a varying number of non-null entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307c6bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "time, Valencia_wind_deg, and Seville are object types, hence, are non-numeric which may have to be changed in data engineering section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "6779384b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 0\n",
       "Madrid_wind_speed          0\n",
       "Valencia_wind_deg          0\n",
       "Bilbao_rain_1h             0\n",
       "Valencia_wind_speed        0\n",
       "Seville_humidity           0\n",
       "Madrid_humidity            0\n",
       "Bilbao_clouds_all          0\n",
       "Bilbao_wind_speed          0\n",
       "Seville_clouds_all         0\n",
       "Bilbao_wind_deg            0\n",
       "Barcelona_wind_speed       0\n",
       "Barcelona_wind_deg         0\n",
       "Madrid_clouds_all          0\n",
       "Seville_wind_speed         0\n",
       "Barcelona_rain_1h          0\n",
       "Seville_pressure           0\n",
       "Seville_rain_1h            0\n",
       "Bilbao_snow_3h             0\n",
       "Barcelona_pressure         0\n",
       "Seville_rain_3h            0\n",
       "Madrid_rain_1h             0\n",
       "Barcelona_rain_3h          0\n",
       "Valencia_snow_3h           0\n",
       "Madrid_weather_id          0\n",
       "Barcelona_weather_id       0\n",
       "Bilbao_pressure            0\n",
       "Seville_weather_id         0\n",
       "Valencia_pressure       2068\n",
       "Seville_temp_max           0\n",
       "Madrid_pressure            0\n",
       "Valencia_temp_max          0\n",
       "Valencia_temp              0\n",
       "Bilbao_weather_id          0\n",
       "Seville_temp               0\n",
       "Valencia_humidity          0\n",
       "Valencia_temp_min          0\n",
       "Barcelona_temp_max         0\n",
       "Madrid_temp_max            0\n",
       "Barcelona_temp             0\n",
       "Bilbao_temp_min            0\n",
       "Bilbao_temp                0\n",
       "Barcelona_temp_min         0\n",
       "Bilbao_temp_max            0\n",
       "Seville_temp_min           0\n",
       "Madrid_temp                0\n",
       "Madrid_temp_min            0\n",
       "load_shortfall_3h          0\n",
       "year                       0\n",
       "month                      0\n",
       "day                        0\n",
       "hour                       0\n",
       "Madrid_temp_range          0\n",
       "Valencia_temp_range        0\n",
       "Seville_temp_range         0\n",
       "Bilbao_temp_range          0\n",
       "Barcelona_temp_range       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #provide the total number of null values appearing in each feature. basically opposite of df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "edb605d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 0\n",
       "Madrid_wind_speed          0\n",
       "Valencia_wind_deg          0\n",
       "Bilbao_rain_1h             0\n",
       "Valencia_wind_speed        0\n",
       "Seville_humidity           0\n",
       "Madrid_humidity            0\n",
       "Bilbao_clouds_all          0\n",
       "Bilbao_wind_speed          0\n",
       "Seville_clouds_all         0\n",
       "Bilbao_wind_deg            0\n",
       "Barcelona_wind_speed       0\n",
       "Barcelona_wind_deg         0\n",
       "Madrid_clouds_all          0\n",
       "Seville_wind_speed         0\n",
       "Barcelona_rain_1h          0\n",
       "Seville_pressure           0\n",
       "Seville_rain_1h            0\n",
       "Bilbao_snow_3h             0\n",
       "Barcelona_pressure         0\n",
       "Seville_rain_3h            0\n",
       "Madrid_rain_1h             0\n",
       "Barcelona_rain_3h          0\n",
       "Valencia_snow_3h           0\n",
       "Madrid_weather_id          0\n",
       "Barcelona_weather_id       0\n",
       "Bilbao_pressure            0\n",
       "Seville_weather_id         0\n",
       "Valencia_pressure       2068\n",
       "Seville_temp_max           0\n",
       "Madrid_pressure            0\n",
       "Valencia_temp_max          0\n",
       "Valencia_temp              0\n",
       "Bilbao_weather_id          0\n",
       "Seville_temp               0\n",
       "Valencia_humidity          0\n",
       "Valencia_temp_min          0\n",
       "Barcelona_temp_max         0\n",
       "Madrid_temp_max            0\n",
       "Barcelona_temp             0\n",
       "Bilbao_temp_min            0\n",
       "Bilbao_temp                0\n",
       "Barcelona_temp_min         0\n",
       "Bilbao_temp_max            0\n",
       "Seville_temp_min           0\n",
       "Madrid_temp                0\n",
       "Madrid_temp_min            0\n",
       "load_shortfall_3h          0\n",
       "year                       0\n",
       "month                      0\n",
       "day                        0\n",
       "hour                       0\n",
       "Madrid_temp_range          0\n",
       "Valencia_temp_range        0\n",
       "Seville_temp_range         0\n",
       "Bilbao_temp_range          0\n",
       "Barcelona_temp_range       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "38b7be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find collumns with nulls and store the index as variable called column_iloc\n",
    "\n",
    "null_mask = df.isnull().any()\n",
    "columns_with_nulls = null_mask[null_mask].index\n",
    "\n",
    "null_column_iloc = df.columns.get_loc(columns_with_nulls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0286383",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Non-Graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "3461961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of null values in '28': 23.60%\n"
     ]
    }
   ],
   "source": [
    "#find the percentage of null values for collumn with nulls\n",
    "\n",
    "null_percentage = (df.iloc[:, null_column_iloc].isnull().sum() / len(df)) * 100\n",
    "print(f\"Percentage of null values in '{null_column_iloc}': {null_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "89943dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015.0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the median of the column\n",
    "df.iloc[:, null_column_iloc].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "761787f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6695.000000\n",
       "mean     1012.051407\n",
       "std         9.506214\n",
       "min       972.666667\n",
       "25%      1010.333333\n",
       "50%      1015.000000\n",
       "75%      1018.000000\n",
       "max      1021.666667\n",
       "Name: Valencia_pressure, dtype: float64"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifiying the statistic summaries of the null column\n",
    "\n",
    "df.iloc[:, null_column_iloc].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "93071e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 0\n",
       "Madrid_wind_speed          0\n",
       "Valencia_wind_deg          0\n",
       "Bilbao_rain_1h             0\n",
       "Valencia_wind_speed        0\n",
       "Seville_humidity           0\n",
       "Madrid_humidity            0\n",
       "Bilbao_clouds_all          0\n",
       "Bilbao_wind_speed          0\n",
       "Seville_clouds_all         0\n",
       "Bilbao_wind_deg            0\n",
       "Barcelona_wind_speed       0\n",
       "Barcelona_wind_deg         0\n",
       "Madrid_clouds_all          0\n",
       "Seville_wind_speed         0\n",
       "Barcelona_rain_1h          0\n",
       "Seville_pressure           0\n",
       "Seville_rain_1h            0\n",
       "Bilbao_snow_3h             0\n",
       "Barcelona_pressure         0\n",
       "Seville_rain_3h            0\n",
       "Madrid_rain_1h             0\n",
       "Barcelona_rain_3h          0\n",
       "Valencia_snow_3h           0\n",
       "Madrid_weather_id          0\n",
       "Barcelona_weather_id       0\n",
       "Bilbao_pressure            0\n",
       "Seville_weather_id         0\n",
       "Valencia_pressure       2068\n",
       "Seville_temp_max           0\n",
       "Madrid_pressure            0\n",
       "Valencia_temp_max          0\n",
       "Valencia_temp              0\n",
       "Bilbao_weather_id          0\n",
       "Seville_temp               0\n",
       "Valencia_humidity          0\n",
       "Valencia_temp_min          0\n",
       "Barcelona_temp_max         0\n",
       "Madrid_temp_max            0\n",
       "Barcelona_temp             0\n",
       "Bilbao_temp_min            0\n",
       "Bilbao_temp                0\n",
       "Barcelona_temp_min         0\n",
       "Bilbao_temp_max            0\n",
       "Seville_temp_min           0\n",
       "Madrid_temp                0\n",
       "Madrid_temp_min            0\n",
       "load_shortfall_3h          0\n",
       "year                       0\n",
       "month                      0\n",
       "day                        0\n",
       "hour                       0\n",
       "Madrid_temp_range          0\n",
       "Valencia_temp_range        0\n",
       "Seville_temp_range         0\n",
       "Bilbao_temp_range          0\n",
       "Barcelona_temp_range       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics of each numerical feature by using the following command:\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f899d",
   "metadata": {},
   "source": [
    "will replace Valencia_pressure null values with median Value if the nulls encompass less than 30% of the whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a716dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional statistical measures that can be calculated are kurtosis and skew.\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the image path as an argument to the Image class\n",
    "\n",
    "# display(Image(filename=\"Skew.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6344531",
   "metadata": {},
   "source": [
    "Entries with high negative skew are: Valencia_pressure, Madrid_pressure, (and Bilbao_weather_id, Madrid_weather_id and Barcelona_weather_id). Those with high positive skew are: Bilbao_rain_1h, Valencia_wind_speed, Bilbao_snow_3h, Seville_rain_3h, Barcelona_pressure, Seville_rain_3h, Madrid_rain_1h, Barcelona_rain_3h, Valencia_snow_3h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c190b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High kurtosis (>3) indicates a large number of outliers and low kurtosis (<3) a lack of outliers\n",
    "df.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac148fbe",
   "metadata": {},
   "source": [
    "Features with high kurtosis i.e. >3, implying a large number of outliers are: Bilbao_rain_1h, Valencia_wind_speed, Barcelona_rain_1h, Seville_rain_1h, Bilbao_snow_3h, Barcelona_pressure, Seville_rain_3h, Valencia_snow_3h, (Madrid_weather_id, Barcelona_weather_id, Seville_weather_id, are IDs relevant in distribution study)\n",
    "\n",
    "Units are not provided, however, a pressure of 3687.564230 for Barcelona would not be representative of weather conditions on earth, atmospheric pressure is typically 1013 hPa. This value would potentially be dropped or replaced in data engineering. \n",
    "\n",
    "This is also the case for the recorded value for Valencia_snow_3h as 4089.323165 (centimetres or inches) is not a realistic recording for snowfall especially in 3h. \n",
    "\n",
    "Furthermore, it is highly unlikely for it to rain 413 millimeters (mm) in just 3 hours under normal weather conditions as the rate of rainfall is often expressed in terms of millimeters per hour (mm/hr). A very heavy rainfall rate might be in the range of 50 to 100 mm/hr during an intense thunderstorm or a severe rainfall event. \n",
    "\n",
    "Entries with high skewness often correspond with values with high kurtosis indicating non-normally distributed. These entries would either be replaced or dropped in data emgineering section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb74182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relevant feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate correlation\n",
    "corr = df.drop('Valencia_pressure', axis='columns').corr()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))  \n",
    "plot_corr(corr, xnames=corr.columns, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbdd20",
   "metadata": {},
   "source": [
    "Strongest correlations, and therefore, most influential features on energy production are min/max temperature of the cities. Wind speed, wind degrees, pressure, rain, snow and cloud cover have less influence (bottom right correlations in matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e80806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43844a",
   "metadata": {},
   "source": [
    "Mean: You should consider using the mean (average) to replace null values when your data is approximately normally distributed. This means that the data is symmetric and bell-shaped. The mean can be sensitive to extreme outliers, so if your data has significant outliers, it might not be the best choice.\n",
    "\n",
    "Median: The median is a robust statistic that is less affected by extreme values or outliers. You should consider using the median to replace null values when your data is skewed or has outliers. If your data has a non-normal distribution, the median can be a more representative measure of central tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "88c03f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' to datetime type\n",
    "df['time'] = pd.to_datetime(df['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "fcc85e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day, and hour as separate features\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "# Drop the original 'time' column\n",
    "df.drop(columns=['time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "32be5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nulls with median based on percentage nulls\n",
    "\n",
    "if null_percentage < 30:\n",
    "    median_value = df.iloc[:, null_column_iloc].median()\n",
    "    df.iloc[:, null_column_iloc].fillna(median_value, inplace=True)\n",
    "else:\n",
    "    df.drop(columns_with_nulls, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dce8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your_data_list = df[:, null_column_iloc].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Duplicates\n",
    "# df = df.drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the \"unnamed\" column by checking its name\n",
    "# unnamed_columns = [col for col in df.columns if 'unnamed' in col.lower()]\n",
    "\n",
    "# Drop the \"unnamed\" column(s)\n",
    "# df.drop(columns=unnamed_columns, inplace=True)\n",
    "\n",
    "# If you want to save the modified DataFrame back to a file\n",
    "# df.to_csv('modified_data.csv', index=False)  # Replace 'modified_data.csv' with the desired file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "84eea17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Madrid_temp_range  Valencia_temp_range  Seville_temp_range  \\\n",
      "0                0.0                  0.0                 0.0   \n",
      "1                0.0                  0.0                 0.0   \n",
      "2                0.0                  0.0                 0.0   \n",
      "3                0.0                  0.0                 0.0   \n",
      "4                0.0                  0.0                 0.0   \n",
      "\n",
      "   Bilbao_temp_range  Barcelona_temp_range  \n",
      "0                0.0                   0.0  \n",
      "1                0.0                   0.0  \n",
      "2                0.0                   0.0  \n",
      "3                0.0                   0.0  \n",
      "4                0.0                   0.0  \n"
     ]
    }
   ],
   "source": [
    "# create new features\n",
    "#Temperate range\n",
    "# df['temp_range'] = df['Madrid_temp_max'] - df['Madrid_temp_min']\n",
    "# df['temp_range'] = df['Valencia_temp_max'] - df['Valencia_temp_min']\n",
    "# df['temp_range'] = df['Seville_temp_max'] - df['Seville_temp_min']\n",
    "# df['temp_range'] = df['Bilbao_temp_max'] - df['Bilbao_temp_min']\n",
    "# df['temp_range'] = df['Barcelona_temp_max'] - df['Barcelona_temp_min']\n",
    "# print(df[['Madrid_temp_range', 'Valencia_temp_range', 'Seville_temp_range', 'Bilbao_temp_range', 'Barcelona_temp_range']].head())\n",
    "\n",
    "df['Madrid_temp_range'] = df['Madrid_temp_max'] - df['Madrid_temp_min']\n",
    "df['Valencia_temp_range'] = df['Valencia_temp_max'] - df['Valencia_temp_min']\n",
    "df['Seville_temp_range'] = df['Seville_temp_max'] - df['Seville_temp_min']\n",
    "df['Bilbao_temp_range'] = df['Bilbao_temp_max'] - df['Bilbao_temp_min']\n",
    "df['Barcelona_temp_range'] = df['Barcelona_temp_max'] - df['Barcelona_temp_min']\n",
    "\n",
    "print(df[['Madrid_temp_range', 'Valencia_temp_range', 'Seville_temp_range', 'Bilbao_temp_range', 'Barcelona_temp_range']].head())\n",
    "#Cloud cover\n",
    "#Solar Intensity: The intensity of sunlight is a key factor. \n",
    "#On a bright, sunny day, with high solar intensity, solar panels will heat up more quickly than on a \n",
    "#cloudy or overcast day with lower solar intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59692724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer existing features\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "1a581784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the last number from values and change the column to a numeric type\n",
    "df['Valencia_wind_deg'] = df['Valencia_wind_deg'].str.extract('(\\d+)', expand=False).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "20139c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the Seville_pressure column to a numeric type and extract only the last number from values \n",
    "\n",
    "# Extract only the last number from values and change the column to a numeric type\n",
    "df['Seville_pressure'] = df['Seville_pressure'].str.extract('(\\d+)', expand=False).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "21fab599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                int64\n",
      "Madrid_wind_speed       float64\n",
      "Valencia_wind_deg       float64\n",
      "Bilbao_rain_1h          float64\n",
      "Valencia_wind_speed     float64\n",
      "Seville_humidity        float64\n",
      "Madrid_humidity         float64\n",
      "Bilbao_clouds_all       float64\n",
      "Bilbao_wind_speed       float64\n",
      "Seville_clouds_all      float64\n",
      "Bilbao_wind_deg         float64\n",
      "Barcelona_wind_speed    float64\n",
      "Barcelona_wind_deg      float64\n",
      "Madrid_clouds_all       float64\n",
      "Seville_wind_speed      float64\n",
      "Barcelona_rain_1h       float64\n",
      "Seville_pressure        float64\n",
      "Seville_rain_1h         float64\n",
      "Bilbao_snow_3h          float64\n",
      "Barcelona_pressure      float64\n",
      "Seville_rain_3h         float64\n",
      "Madrid_rain_1h          float64\n",
      "Barcelona_rain_3h       float64\n",
      "Valencia_snow_3h        float64\n",
      "Madrid_weather_id       float64\n",
      "Barcelona_weather_id    float64\n",
      "Bilbao_pressure         float64\n",
      "Seville_weather_id      float64\n",
      "Valencia_pressure       float64\n",
      "Seville_temp_max        float64\n",
      "Madrid_pressure         float64\n",
      "Valencia_temp_max       float64\n",
      "Valencia_temp           float64\n",
      "Bilbao_weather_id       float64\n",
      "Seville_temp            float64\n",
      "Valencia_humidity       float64\n",
      "Valencia_temp_min       float64\n",
      "Barcelona_temp_max      float64\n",
      "Madrid_temp_max         float64\n",
      "Barcelona_temp          float64\n",
      "Bilbao_temp_min         float64\n",
      "Bilbao_temp             float64\n",
      "Barcelona_temp_min      float64\n",
      "Bilbao_temp_max         float64\n",
      "Seville_temp_min        float64\n",
      "Madrid_temp             float64\n",
      "Madrid_temp_min         float64\n",
      "load_shortfall_3h       float64\n",
      "year                      int64\n",
      "month                     int64\n",
      "day                       int64\n",
      "hour                      int64\n",
      "Madrid_temp_range       float64\n",
      "Valencia_temp_range     float64\n",
      "Seville_temp_range      float64\n",
      "Bilbao_temp_range       float64\n",
      "Barcelona_temp_range    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#SANITY CHECK\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at feature distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "2344b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "X = df.drop(columns=['load_shortfall_3h'])  # Features (exclude the target)\n",
    "y = df['load_shortfall_3h']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "9c58df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets and features dataset\n",
    "\n",
    "#Split the data into training and validation sets\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "20d073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one or more ML models\n",
    "#Create a basic ML model (Linear Regression in this example)\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "f92b0543",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\Regression sprint 2307FTDS Team DN1\\Advanced-Regression-Starter-Data\\starter-notebook.ipynb Cell 53\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/Regression%20sprint%202307FTDS%20Team%20DN1/Advanced-Regression-Starter-Data/starter-notebook.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model on the training data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Workspace/Regression%20sprint%202307FTDS%20Team%20DN1/Advanced-Regression-Starter-Data/starter-notebook.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    649\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m )\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21618b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more ML models\n",
    "#model accuracy: variable selection notebook\n",
    "\n",
    "# Evaluate the accuracy of the ML model\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "print(\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
